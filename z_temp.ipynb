{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd29d3de8f04e08adc3e270830b392a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d27226f0715487fa47acdbfc5dc5fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/2.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8337464a99b54d9783156e8d286af91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "relations_for_final.pickle:   0%|          | 0.00/14.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0050fc478dea4adb9d29641f9fd2df70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dbpedia_2015_undirected.pickle:   0%|          | 0.00/6.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1954c44ea47e4ba093f1484cf0da74c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "factkg.zip:   0%|          | 0.00/5.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468c4f4cbe0e41b0b6e43c74fbf9d010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dbpedia_2015_undirected_light.pickle:   0%|          | 0.00/1.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/namb/hoangpv4/kg_fact_checking/data'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot_download(\n",
    "  \"hoang14/factkg\",\n",
    "  repo_type=\"dataset\",\n",
    "  token=\"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\",\n",
    "  local_dir=\"/home/namb/hoangpv4/kg_fact_checking/data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files have been extracted to /home/namb/hoangpv4/kg_fact_checking/data\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "# Specify the path to the zip file and the directory to extract to\n",
    "zip_file_path = \"/home/namb/hoangpv4/kg_fact_checking/data/factkg.zip\"\n",
    "extract_to_path = \"/home/namb/hoangpv4/kg_fact_checking/data\"\n",
    "\n",
    "# Open the zip file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    # Extract all the contents of the zip file\n",
    "    zip_ref.extractall(extract_to_path)\n",
    "\n",
    "print(f\"Files have been extracted to {extract_to_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from src.utils import DataUtils\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\", token=\"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg = DataUtils.load_data(\n",
    "    \"/Users/phamhoang1408/Desktop/graph_checking/data/dbpedia_2015_undirected_light.pickle\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = list(kg.keys())[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Antonio_Vivaldi__2'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1036800.0\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\"', '103', '680', '0', '.', '0', '\"']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity = random.choice(entities)\n",
    "print(entity)\n",
    "tokenizer.tokenize(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['103', '680', '0', '.', '0']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(clean_entity(entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataUtils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dev \u001b[38;5;241m=\u001b[39m \u001b[43mDataUtils\u001b[49m\u001b[38;5;241m.\u001b[39mload_data(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/phamhoang1408/Desktop/graph_checking/data/processed_factkg/factkg_dev.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m test \u001b[38;5;241m=\u001b[39m DataUtils\u001b[38;5;241m.\u001b[39mload_data(\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/phamhoang1408/Desktop/graph_checking/data/processed_factkg/factkg_test.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataUtils' is not defined"
     ]
    }
   ],
   "source": [
    "dev = DataUtils.load_data(\n",
    "    \"/Users/phamhoang1408/Desktop/graph_checking/data/processed_factkg/factkg_dev.json\"\n",
    ")\n",
    "test = DataUtils.load_data(\n",
    "    \"/Users/phamhoang1408/Desktop/graph_checking/data/processed_factkg/factkg_test.json\"\n",
    ")\n",
    "\n",
    "types = set()\n",
    "for sample in dev:\n",
    "    types.update(sample[\"types\"])\n",
    "types\n",
    "\n",
    "partition_types = {\"num1\", \"multi claim\", \"existence\", \"multi hop\", \"negation\"}\n",
    "partitions = {k: [] for k in partition_types}\n",
    "for sample in dev:\n",
    "    valid = False\n",
    "    for t in sample[\"types\"]:\n",
    "        if t in partition_types:\n",
    "            partitions[t].append(sample)\n",
    "            valid = True\n",
    "    if not valid:\n",
    "        raise ValueError(\"Invalid sample\")\n",
    "\n",
    "\n",
    "def partition_data(data, num_sample_per_partition=-1, seed=42):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    partition_types = {\"num1\", \"multi claim\", \"existence\", \"multi hop\", \"negation\"}    \n",
    "    partitions = {k: [] for k in partition_types}\n",
    "    for sample in data:\n",
    "        valid = False\n",
    "        for t in sample[\"types\"]:\n",
    "            if t in partition_types:\n",
    "                partitions[t].append(sample)\n",
    "                valid = True\n",
    "        if not valid:\n",
    "            raise ValueError(\"Invalid sample\")\n",
    "    \n",
    "    if num_sample_per_partition > 0:\n",
    "        for k, v in partitions.items():\n",
    "            random.seed(seed)\n",
    "            partitions[k] = random.sample(v, num_sample_per_partition)\n",
    "\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"content\": \"Stephen rides his bicycle to church. During the first third of his trip, he travels at a speed of x miles per hour. During the second third of his trip, riding uphill, he travels a speed of 12 miles per hour. During the last third of his trip, he rides downhill at a speed of 20 miles per hour. If each third of his trip takes 15 minutes, what is the distance Stephen rides his bicycle to church, in miles?\\nIf we know the answer to the above question is 12, what is the value of unknown variable x?\",\n",
    "            \"role\": \"user\",\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"We know that each third of Stephen's trip takes 15 minutes, or 15/60 = 1/4 hour.\\nDuring the first third of his trip, Stephen travels at a speed of x miles per hour, so he covers a distance of (1/4) * x = x/4 miles.\\nDuring the second third of his trip, riding uphill, Stephen travels at a speed of 12 miles per hour, so he covers a distance of (1/4) * 12 = 3 miles.\\nDuring the last third of his trip, riding downhill, Stephen travels at a speed of 20 miles per hour, so he covers a distance of (1/4) * 20 = 5 miles.\\nThe total distance Stephen rides his bicycle to church is the sum of the distances covered in each third: x/4 + 3 + 5 = x/4 + 8 miles.\\nWe are given that the total distance is 12 miles, so we can write: x/4 + 8 = 12.\\nSubtracting 8 from both sides, we get: x/4 = 4.\\nMultiplying both sides by 4, we get: x = 16.\\nThe value of x is 16.\\n#### 16\\nThe answer is: 16\",\n",
    "            \"role\": \"assistant\",\n",
    "        },\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Translated Conversation ###\n",
      "\n",
      "<USER>: Stephen đi xe đạp đến nhà thờ. Trong phần đầu tiên của chuyến đi, anh ta đi với tốc độ x dặm mỗi giờ. Trong phần thứ hai của chuyến đi, đi lên đồi, anh ta đi với tốc độ 12 dặm mỗi giờ. Trong phần cuối cùng của chuyến đi, anh ta đi xuống đồi với tốc độ 20 dặm mỗi giờ. Nếu mỗi phần của chuyến đi mất 15 phút, khoảng cách Stephen đi xe đạp đến nhà thờ là bao nhiêu, tính bằng dặm?\n",
      "Nếu chúng ta biết câu trả lời cho câu hỏi trên là 12, giá trị của biến x là gì?\n",
      "\n",
      "<ASSISTANT>: Chúng ta biết rằng mỗi phần của chuyến đi của Stephen mất 15 phút, hoặc 15/60 = 1/4 giờ.\n",
      "Trong phần đầu tiên của chuyến đi, Stephen đi với tốc độ x dặm mỗi giờ, vì vậy anh ta đi được một khoảng cách là (1/4) * x = x/4 dặm.\n",
      "Trong phần thứ hai của chuyến đi, đi lên đồi, Stephen đi với tốc độ 12 dặm mỗi giờ, vì vậy anh ta đi được một khoảng cách là (1/4) * 12 = 3 dặm.\n",
      "Trong phần cuối cùng của chuyến đi, đi xuống đồi, Stephen đi với tốc độ 20 dặm mỗi giờ, vì vậy anh ta đi được một khoảng cách là (1/4) * 20 = 5 dặm.\n",
      "Tổng khoảng cách Stephen đi xe đạp đến nhà thờ là tổng của các khoảng cách đi được trong mỗi phần: x/4 + 3 + 5 = x/4 + 8 dặm.\n",
      "Chúng ta được cho biết tổng khoảng cách là 12 dặm, vì vậy chúng ta có thể viết: x/4 + 8 = 12.\n",
      "Trừ 8 từ cả hai bên, chúng ta có: x/4 = 4.\n",
      "Nhân cả hai bên với 4, chúng ta có: x = 16.\n",
      "Giá trị của x là 16.\n",
      "#### 16\n",
      "Câu trả lời là: 16\n"
     ]
    }
   ],
   "source": [
    "from together import Together\n",
    "\n",
    "DEFAULT_MODEL = \"meta-llama/Meta-Llama-3.1-405B-Instruct-Lite-Pro\"\n",
    "def get_completion(prompt, model=DEFAULT_MODEL):\n",
    "    try:\n",
    "        client = Together(\n",
    "            api_key=\"97a11123ede5f0657785added8d36067eae9834fa30084338c6cb5dcf909f0fd\"\n",
    "        )\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "conversation = convert_to_conversation(point)\n",
    "\n",
    "output = get_completion(PROMPT.replace(\"{{conversation}}\", conversation))\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from openai import OpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "TOKENIZER_PATH = \"path\"\n",
    "DEFAULT_MODEL = \"model_name\"\n",
    "SERVER_HOST = \"\"\n",
    "TEMPERATURE = 0.0\n",
    "TOP_P = 0.9\n",
    "MAX_TOKENS = 1024\n",
    "\n",
    "\n",
    "def get_completion_vllm(\n",
    "    input_prompt,\n",
    "    system_prompt=None,\n",
    "    model=DEFAULT_MODEL,\n",
    "    temperature=TEMPERATURE,\n",
    "    top_p=TOP_P,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    server_host=SERVER_HOST,\n",
    "):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "    client = OpenAI(\n",
    "        api_key=\"EMPTY\",\n",
    "        base_url=f\"{SERVER_HOST}/v1\",\n",
    "    )\n",
    "    conversation = []\n",
    "    if system_prompt is not None:\n",
    "        conversation.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    conversation.append({\"role\": \"user\", \"content\": input_prompt})\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        conversation, add_generation_propmpt=True, tokenize=False\n",
    "    )\n",
    "    try:\n",
    "        response = client.completions.create(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            seed=0,\n",
    "            top_p=top_p,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def multi_process_task_dict(task_dictionary, num_workers=1, show_progress=True):\n",
    "    final_results = {}\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = []\n",
    "        for id_, task in task_dictionary.items():\n",
    "            to_submit_task = lambda id_=id_, task=task: {\n",
    "                \"id\": id_,\n",
    "                \"task_result\": task(),\n",
    "            }\n",
    "            future = executor.submit(to_submit_task)\n",
    "        if show_progress:\n",
    "            with tqdm(total=len(task_dictionary)) as pbar:\n",
    "                for future in as_completed(futures):\n",
    "                    result = future.result()\n",
    "                    final_results[result[\"id\"]] = result[\"task_result\"]\n",
    "                    pbar.update(1)\n",
    "        else:\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                final_results[result[\"id\"]] = result[\"task_result\"]\n",
    "\n",
    "    return final_results\n",
    "\n",
    "\n",
    "PROMPT_TRANSLATE_METAMATHQA_50k = \"\"\"\n",
    "### Translate the following conversation to Vietnamese.\n",
    "\n",
    "### Notes:\n",
    "1) The translated text should be native, natural, and fluent.\n",
    "2) Do not translate special keywords including: <USER>, <ASSISTANT> and keep the format of the conversation intact.\n",
    "3) The conversation can contain text content, mathematical and logical expressions, you should translate only the text content and keep the mathematical and logical expressions intact.\n",
    "4) Only return the translated conversation and NOTHING else, start the translated content with special marker ### Translated Conversation ###\n",
    "\n",
    "### Conversation:\n",
    "{{conversation}}\n",
    "\"\"\".strip()\n",
    "\n",
    "def task_translate_metamathqa_50k(sample):\n",
    "    def convert_to_conversation(sample):\n",
    "        role_mapper = {\"user\": \"<USER>\", \"assistant\": \"<ASSISTANT>\"}\n",
    "        text = \"\"\n",
    "        for message in sample[\"messages\"]:\n",
    "            role = role_mapper[message[\"role\"]]\n",
    "            text += f\"{role}: {message['content']}\\n\"\n",
    "        return text\n",
    "\n",
    "    def convert_formatted_text_to_json(formatted_text):\n",
    "        lines = [line.strip() for line in formatted_text.split(\"\\n\") if line.strip()]\n",
    "        start_idx = 0\n",
    "        if \"### Translated Conversation ###\" in lines[0]:\n",
    "            start_idx = 1\n",
    "\n",
    "        messages = []\n",
    "        for line in lines[start_idx:]:\n",
    "            match = re.match(r\"<(USER|ASSISTANT)>:\\s*(.*)\", line, re.IGNORECASE)\n",
    "            if match:\n",
    "                role, content = match.groups()\n",
    "                role = role.lower()\n",
    "                messages.append({\"role\": role, \"content\": content.strip()})\n",
    "\n",
    "        return {\"messages\": messages}\n",
    "\n",
    "    \n",
    "\n",
    "    conversation = convert_to_conversation(sample)\n",
    "    prompt = PROMPT_TRANSLATE_METAMATHQA_50k.replace(\"{{conversation}}\", conversation)\n",
    "    response = None\n",
    "    try:\n",
    "        response = get_completion_vllm(prompt)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "    sample[\"translated_message_text\"] = response\n",
    "    sample[\"translated_messages\"] = convert_formatted_text_to_json(response)\n",
    "    return sample\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"HuggingFaceTB/smoltalk\", \"metamathqa-50k\")['train']\n",
    "task_dict = {idx: lambda: task_translate_metamathqa_50k(sample) for idx, sample in enumerate(dataset)}\n",
    "results = multi_process_task_dict(task_dict, num_workers=20, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and parse a yaml file\n",
    "import yaml\n",
    "\n",
    "with open(\"/Users/phamhoang1408/Desktop/graph_checking/temp.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['token1', 'token2', 'token3', 'token4', 'token5']}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
